{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Downloads/ex1_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airport Code</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CDG</td>\n",
       "      <td>49.012798</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHC</td>\n",
       "      <td>-43.489399</td>\n",
       "      <td>172.531998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DYR</td>\n",
       "      <td>64.734901</td>\n",
       "      <td>177.740997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EWR</td>\n",
       "      <td>40.692501</td>\n",
       "      <td>-74.168701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HNL</td>\n",
       "      <td>21.318701</td>\n",
       "      <td>-157.921997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OME</td>\n",
       "      <td>64.512199</td>\n",
       "      <td>-165.445007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ONU</td>\n",
       "      <td>-20.650000</td>\n",
       "      <td>-178.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PEK</td>\n",
       "      <td>40.080101</td>\n",
       "      <td>116.584999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Airport Code        Lat        Long\n",
       "0          CDG  49.012798    2.550000\n",
       "1          CHC -43.489399  172.531998\n",
       "2          DYR  64.734901  177.740997\n",
       "3          EWR  40.692501  -74.168701\n",
       "4          HNL  21.318701 -157.921997\n",
       "5          OME  64.512199 -165.445007\n",
       "6          ONU -20.650000 -178.699997\n",
       "7          PEK  40.080101  116.584999"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {t[0]:(t[1],t[2]) for t in df.to_numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDG': (49.0127983093, 2.54999995232),\n",
       " 'CHC': (-43.4893989562988, 172.53199768066398),\n",
       " 'DYR': (64.7349014282227, 177.740997314453),\n",
       " 'EWR': (40.6925010681152, -74.168701171875),\n",
       " 'HNL': (21.3187007904053, -157.92199707031202),\n",
       " 'OME': (64.5121994018555, -165.445007324219),\n",
       " 'ONU': (-20.6499996185303, -178.699996948242),\n",
       " 'PEK': (40.0801010131836, 116.584999084473)}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would compute distances using the <b>[Haversine distance](https://en.wikipedia.org/wiki/Haversine_formula)</b> formula for latitude and longitude distance between two points on a sphere. \n",
    "* [Source 1](https://andrew.hedges.name/experiments/haversine/) \n",
    "* [Source 2](https://gist.github.com/rochacbruno/2883505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def havdist(a,b):\n",
    "    \n",
    "    # function returns distance in kilometers\n",
    "    \n",
    "    lat1 = a[0]\n",
    "    lat2 = b[0]\n",
    "    \n",
    "    lon1 = a[1]\n",
    "    lon2 = b[1]\n",
    "    \n",
    "    ### average radius in km of Earth assuming it's a perfect sphere\n",
    "    R = 6373 \n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a),math.sqrt(1-a))\n",
    "    d = R*c\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19037.286815140535"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "havdist(coords['CDG'],coords['CHC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_airports(coords,origin):\n",
    "     \n",
    "    candidates = [k for k in coords.keys() if k != origin]\n",
    "     \n",
    "    airports = {}\n",
    "    for c in candidates:\n",
    "        \n",
    "        airports[c] = havdist(coords[origin],coords[c])\n",
    "    \n",
    "    nearest_airports = {a:airports[a] for a in sorted(airports, key=airports.get)}\n",
    "    \n",
    "    return(nearest_airports)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DYR': 4703.290435510359,\n",
       " 'OME': 5502.753636192683,\n",
       " 'HNL': 8134.981676799219,\n",
       " 'CDG': 8191.316316210715,\n",
       " 'ONU': 9508.285362066754,\n",
       " 'CHC': 10856.149489614194,\n",
       " 'EWR': 10971.319185764798}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_airports(coords,'PEK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem appears to be an A/B testing exercise, where we will be testing Bernoulli trials i.e. conversion rate using a pooled <b> [Z-test of proportions](https://stattrek.com/hypothesis-test/difference-in-proportions.aspx) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "df = pd.read_csv('~/Downloads/ex2_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_type</th>\n",
       "      <th>variant</th>\n",
       "      <th>discount</th>\n",
       "      <th>total_views</th>\n",
       "      <th>price_freezes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>android</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>False</td>\n",
       "      <td>6010</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>android</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>True</td>\n",
       "      <td>331</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>android</td>\n",
       "      <td>Champion</td>\n",
       "      <td>False</td>\n",
       "      <td>1084</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>android</td>\n",
       "      <td>Champion</td>\n",
       "      <td>True</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>False</td>\n",
       "      <td>6905</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>True</td>\n",
       "      <td>1986</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Champion</td>\n",
       "      <td>False</td>\n",
       "      <td>6576</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Champion</td>\n",
       "      <td>True</td>\n",
       "      <td>2054</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  device_type     variant  discount  total_views  price_freezes\n",
       "0     android  Challenger     False         6010            189\n",
       "1     android  Challenger      True          331             16\n",
       "2     android    Champion     False         1084             23\n",
       "3     android    Champion      True           54              3\n",
       "4         iOS  Challenger     False         6905            336\n",
       "5         iOS  Challenger      True         1986            196\n",
       "6         iOS    Champion     False         6576            266\n",
       "7         iOS    Champion      True         2054            161"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating pooled proportion for hypothesis testing\n",
    "\n",
    "p0 = df.sum().price_freezes/df.sum().total_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_freeze_rate'] = df.price_freezes/df.total_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df.variant == 'Champion']\n",
    "b = df.loc[df.variant == 'Challenger']\n",
    "\n",
    "combined = a.merge(b,how='inner',left_on=['device_type','discount'],right_on=['device_type','discount'],suffixes=('_a','_b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['diff'] = combined.price_freeze_rate_a - combined.price_freeze_rate_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a small function to compute the test statistic and one-sided p-value\n",
    "\n",
    "def z_test(p0,p1,p2,n1,n2):\n",
    "    z = (p2 - p1)/(math.sqrt(p0*(1-p0)*((1/n1) + 1/n2)))\n",
    "    return(z)\n",
    "\n",
    "def pvalue(z):\n",
    "    p = scipy.stats.norm.sf(abs(z))\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['z_stat'] =  combined.apply(lambda row: z_test(p0,row['price_freeze_rate_a'],row['price_freeze_rate_b'],row['total_views_a'],row['total_views_b']),axis=1)\n",
    "combined['one_sided_p'] = combined.z_stat.apply(lambda z: pvalue(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_type</th>\n",
       "      <th>variant_a</th>\n",
       "      <th>discount</th>\n",
       "      <th>total_views_a</th>\n",
       "      <th>price_freezes_a</th>\n",
       "      <th>price_freeze_rate_a</th>\n",
       "      <th>variant_b</th>\n",
       "      <th>total_views_b</th>\n",
       "      <th>price_freezes_b</th>\n",
       "      <th>price_freeze_rate_b</th>\n",
       "      <th>diff</th>\n",
       "      <th>z_stat</th>\n",
       "      <th>one_sided_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>android</td>\n",
       "      <td>Champion</td>\n",
       "      <td>False</td>\n",
       "      <td>1084</td>\n",
       "      <td>23</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>6010</td>\n",
       "      <td>189</td>\n",
       "      <td>0.031448</td>\n",
       "      <td>-0.010230</td>\n",
       "      <td>1.456007</td>\n",
       "      <td>0.072695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>android</td>\n",
       "      <td>Champion</td>\n",
       "      <td>True</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>331</td>\n",
       "      <td>16</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>-0.230959</td>\n",
       "      <td>0.408673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Champion</td>\n",
       "      <td>False</td>\n",
       "      <td>6576</td>\n",
       "      <td>266</td>\n",
       "      <td>0.040450</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>6905</td>\n",
       "      <td>336</td>\n",
       "      <td>0.048660</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>2.237927</td>\n",
       "      <td>0.012613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Champion</td>\n",
       "      <td>True</td>\n",
       "      <td>2054</td>\n",
       "      <td>161</td>\n",
       "      <td>0.078384</td>\n",
       "      <td>Challenger</td>\n",
       "      <td>1986</td>\n",
       "      <td>196</td>\n",
       "      <td>0.098691</td>\n",
       "      <td>-0.020307</td>\n",
       "      <td>3.030651</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  device_type variant_a  discount  total_views_a  price_freezes_a  \\\n",
       "0     android  Champion     False           1084               23   \n",
       "1     android  Champion      True             54                3   \n",
       "2         iOS  Champion     False           6576              266   \n",
       "3         iOS  Champion      True           2054              161   \n",
       "\n",
       "   price_freeze_rate_a   variant_b  total_views_b  price_freezes_b  \\\n",
       "0             0.021218  Challenger           6010              189   \n",
       "1             0.055556  Challenger            331               16   \n",
       "2             0.040450  Challenger           6905              336   \n",
       "3             0.078384  Challenger           1986              196   \n",
       "\n",
       "   price_freeze_rate_b      diff    z_stat  one_sided_p  \n",
       "0             0.031448 -0.010230  1.456007     0.072695  \n",
       "1             0.048338  0.007217 -0.230959     0.408673  \n",
       "2             0.048660 -0.008210  2.237927     0.012613  \n",
       "3             0.098691 -0.020307  3.030651     0.001220  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at differences among all sub-populations to control for the confounding variables\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = df.groupby(['variant']).sum()[['total_views','price_freezes']]\n",
    "overall['rate'] = overall.price_freezes/overall.total_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_views</th>\n",
       "      <th>price_freezes</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Challenger</th>\n",
       "      <td>15232</td>\n",
       "      <td>737</td>\n",
       "      <td>0.048385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Champion</th>\n",
       "      <td>9768</td>\n",
       "      <td>453</td>\n",
       "      <td>0.046376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_views  price_freezes      rate\n",
       "variant                                         \n",
       "Challenger        15232            737  0.048385\n",
       "Champion           9768            453  0.046376"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall\n",
    "### p0 is the same as before, as it's pooled probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall z-score is 0.727910616681884\n",
      "Overall p-value is 0.23333415188704776\n"
     ]
    }
   ],
   "source": [
    "### Looking at overall probabilities to answer the first question\n",
    "\n",
    "overall_z = z_test(p0,0.046376,0.048385,9768,15232)\n",
    "print('Overall z-score is {}'.format(overall_z))\n",
    "print('Overall p-value is {}'.format(pvalue(overall_z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_views</th>\n",
       "      <th>price_freezes</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iOS</th>\n",
       "      <td>8891</td>\n",
       "      <td>532</td>\n",
       "      <td>0.059836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             total_views  price_freezes      rate\n",
       "device_type                                      \n",
       "iOS                 8891            532  0.059836"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To answer the last question -- filtering on the Challenger model, where I'll use the pooled freeze rate to compute\n",
    "### the confidence interval likelihood of observing a given number of price freezes\n",
    "\n",
    "device = df.loc[(df.variant == 'Challenger') & (df.device_type == 'iOS')].groupby(['device_type']).sum()[['total_views','price_freezes']]\n",
    "device['rate'] = device.price_freezes/device.total_views\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected price freezes using Challenger model for 10,000 views is 598.0\n"
     ]
    }
   ],
   "source": [
    "np = device['rate'][0]*10000\n",
    "print('Expected price freezes using Challenger model for 10,000 views is {}'.format(round(np,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Z test statistic is -4.146931246080383\n",
      "P-value is 1.6848059696943557e-05\n"
     ]
    }
   ],
   "source": [
    "# We assume we can use the Z-statistic because sample size is large enough\n",
    "\n",
    "p_hat = 500/10000\n",
    "p_expected = device['rate'][0] \n",
    "\n",
    "z = (p_hat - p_expected)/math.sqrt(p_expected*(1-p_expected)*(1/10000))\n",
    "\n",
    "print('The Z test statistic is {}'.format(z))\n",
    "print('P-value is {}'.format(pvalue(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Z test statistic is 0.06923407226160429\n",
      "P-value is 0.4724016512116393\n"
     ]
    }
   ],
   "source": [
    "p_hat = 600/10000\n",
    "p_expected = device['rate'][0] \n",
    "\n",
    "z = (p_hat - p_expected)/math.sqrt(p_expected*(1-p_expected)*(1/10000))\n",
    "\n",
    "print('The Z test statistic is {}'.format(z))\n",
    "print('P-value is {}'.format(pvalue(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05983578900011247"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device['rate'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the questions:\n",
    "\n",
    "1. Based on the above, there is a <b>23%</b> chance we'd commit a Type I error and incorrectly reject the null hypothesis (that they're the same) in favor of the alternative hypothesis (that the Challenger model is better). Conversely, this means that we have a confidence of <b>77%</b> probability that the Challenger model is better compared to the Champion model.\n",
    "\n",
    "\n",
    "\n",
    "2. Based on my answer to the first question, I am <i>not</i> comfortable rolling out the new model for every customer. <i>However</i>, I would like to note that <b> iOS users </b> across the board responded in a statistically significantly positive way to the new Challenger model at p-values of 1.2% (discount) and 0.1% (non-discount). Thus, I <b> would be comfortable rolling out the new Challenger model to iOS users </b> and then troubleshoot and try to identify the root cause behind the Android user population's relative under-response to the new model... perhaps it's related to the implementation on Android devices.\n",
    "\n",
    "\n",
    "\n",
    "3. Given the current expected price freeze rate of <b>5.98%</b> for the iOS population that we've observed for these users and the large hypothetical sample size of 10,000 views, we can be very confident that we'll observe at least 500 price freezes for a sample size of 10,000 with the Challenger model (with probability close to 1). Given the same parameters, we can expect to observe  600 price freezes with a probability of <b> close to 47%</b>. This does, however, assume that the proportion of discount impressions among the test population is the same as the mix given here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I'd transform this data using one-hot encoding of flight attributes. Specifically, I'd create a vectorization which contained the following attributes:\n",
    "\n",
    "    i. <b>is_round_trip </b> dummy variable for whether a trip is one-way or round-trip\n",
    "    \n",
    "    ii. <b>departure_date</b> variable, transformed to an integer like UNIX Epoch time\n",
    "        \n",
    "    iii. <b>departure_{city/airport}_{CODE}</b> that had every city/airport code on the platform that was 1 if the user's search had the location as the origin, 0 if it didn't\n",
    "    \n",
    "    iv. <b>destination_{city/airport}_{CODE}</b> - same logic for departure, but for destination\n",
    "    \n",
    "    v. <b>stay_duration</b> - stay length as given in the file, only populated for round-trips it looks like -- we could pad the missing values for the one-way trips with a median, or we could do separate vectorizations for round-trip and one-way trips that do/don't have this attribute respectively (in which case we wouldn't need the \"is_round_trip\" dummy variable given in i. either\n",
    "    \n",
    "    vi. <b>return_date</b> same as departure date, but since it's only populated for round-trip searches it might make sense to split one-way/round-trip vectorizations \n",
    "    \n",
    "    \n",
    "I would bucket searches that satisfied the following criteria, averaging the vectors created above based on the criteria below to return a single \"smoothed\" search-trip vector:\n",
    "\n",
    "1. <b> Same user_id </b>\n",
    "2. <b> Same departure location (city/airport) </b>\n",
    "3. <b>Departure date within a given range </b> -- say no greater than 3-7 days difference (the appropriate threshold would have to be determined from exploratory data analysis of the empirical proper threshold for distinct trip search buckets) -- from other searches they've conducted that meet the first 2 criteria\n",
    "\n",
    "\n",
    "You can use methods such as <b>where</b>, <b>get_dummies</b>, <b> map</b>, or <b> apply</b> to accomplish the above. As I mentioned before, it might make sense to split the vectorizations into two separate processes for the one-way and round-trip searches since the number of available attributes may differ. On the other hand, we might get away with imputing an average/median value within each of the buckets above regardless of round-trip/one-way status -- users may, after all, choose to only search a segment of their entire trip on the platform (e.g. one-way SFO -> LAX on Hopper, booking LAX -> DFW on another platform, later returning to conduct a search for round-trip SFO -> DFW in its entirety) -- these trips could be bucketed and we could impute the round-trip values to the one-way trip values in these cases. This method isn't perfect, but I believe it'd get us closer to grouping searches and similar trips\n",
    "\n",
    "2. <b> To compute overall similarity I'd use cosine similarity </b> which is analogous to the \"correlation\", which is the dot product of the search vectorizations divided by their respective norms. <b>[Source](https://en.wikipedia.org/wiki/Cosine_similarity)</b>\n",
    "\n",
    "\n",
    "3. It would probably enhance trip similarity estimates if we included any intermediate layovers (not essential), arrival dates, departure/arrival times + hours, whether the purchased flights were for the user themselves or family/colleagues for business etc. It would also be great if we had carrier information, as folks fly with many different airlines, many of which leave/arrive on the same day from/to the same places\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the success of the \"Price Drop\" feature, I'd first want to discuss with the product team whether the proposed KPIs effectively captured the change they were hoping to implement:\n",
    "\n",
    "* Daily/weekly bookings\n",
    "* Daily/weekly/\"Price drop\" usage rates as a % of total weekly bookings\n",
    "* Gross daily/weekly booking dollars net \"Price Drop\" refunds (also this metric using Hopper's commission net \"Price Drop\" refunds)\n",
    "\n",
    "We'd want to ensure that we created a properly powered A/B test that rolled out the split test to a stratified, relatively balanced set of users (we don't want all of our power-users ending up in one test bucket) that ran for say... 2-4 weeks (based on Hopper's traffic, booking volume, etc. to ensure sufficient statistical power and the lag necessary for the \"Price Drop\" feature to be used)\n",
    "\n",
    "The metrics above would help capture\n",
    "\n",
    "   * whether overall bookings have increased relative to the control due to the introduction of the feature\n",
    "   * whether the feature was being utilized\n",
    "   * whether the feature was good for Hopper's business\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
